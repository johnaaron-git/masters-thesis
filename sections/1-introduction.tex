Graphs are a flexible and powerful abstraction for representing complex and interdependent relationships between entities. Dynamic graphs extend this capability, facilitating time-aware applications in a range of fields including social networks, fraud systems, distributed hardware monitoring, and biological systems. The less rigid constraints of dynamic graph data are especially important in fields where the nature of its relationships (or behavior over time) is unclear.

Methods for processing and understanding relationships in graph datasets have grown to include machine learning, using deep learning neural networks to find patterns and irregularities in data. These neural networks offer robust, scalable alternatives to traditional statistical or rule-based algorithms at the expense of transparency.

There is strong demand from industry, academia, and the broader public for both accountability and increased understanding of machine learning models being rapidly adopted across subject domains\cite{weber_quantifying_2021}\cite{rueda_just_2022}\cite{deeks_judicial_2019}. All machine learning models that are not intrinsically interpretable are subject to scrutiny; how can we trust the fairness and accuracy of machine learning decision-making if we are unable to explain the process?

Explainability models attempt to address this opaqueness by explaining the results produced from these algorithms post-hoc. This is crucial for fraud detection, recommendation systems, and predictive analytics where false positives or faulty trend data can create significant real-world consequences. Despite the rapid growth of novel machine learning techniques for anomaly detection and increased demand for dynamic graph compatibility, there is a lack of comprehensive surveys for dynamic explainability in the field. This paper aims to address that gap by exploring the breadth of deep learning explainability in dynamic graph anomaly detection, recommending a consistent and reproducible experimental design, and identifying key challenges and opportunities for future research.

\subsection{Challenges}
In addition to the complexities presented by machine learning models, the added layer of model explanation introduces its own problems: there is an inherent \textit{trade-off between accuracy and explainability}. Highly accurate models must increase complexity by design to more completely capture the information and relationships stored in graph data. Capturing time-aware feature interdependence can be understood as an extension of this principle. The degree to which this inverse relationship can be mitigated is a matter of debate\cite{bell_its_2022}, and public preference between the two varies\cite{van_der_veer_trading_2021}.

Explanation research has legal implications for both \textit{data privacy} and \textit{decision-making liability}. A look at commonly benchmarked datasets (e.g., social networks, customer-vendor networks, digital transaction maps) shows that many are generated by private companies or government agencies that contain proprietary interests\cite{ma_comprehensive_2021}. Identifiable features must be masked and sanitized before public release, but explanations can reveal underlying trend data or dimensions that would otherwise be obfuscated. Additionally, Grant and Wischik posit that there is litigation risk associated with black-box training and decision-making, where no binding definition of "meaningful information" exists for automated decisions. The uncertainty gap in an explanation both invites a challenge of bias and the possibility of indefensible false-positives based on real material damages to plaintiffs. "Systems design ... reliability and oversight ... [these] are not concepts that will be helpful when defending against the claim: 'You didn't provide me with a meaningful explanation of your machine learning system's decision\cite{grant_show_2020}'".

Model-specific explainability constraints may also impede the standardization of dynamic graph data structures. Forming time-aware graph data is labor intensive and specific to the analysis and explanatory methods applied.

It is possible that a combination of these privacy considerations, limited storage standardization, and the complexity involved with collecting time-aware anomalous graph data has also led to a \textit{lack of publicly available datasets}. This could be attributed to the novelty of the field; model explanation is still an emerging field of study: publication data indicates that most explainability work is from 2021 or newer[\ref{fig:publications-keywords}].

\subsection{Contribution and Scope}
This survey provides an up-to-date review of dynamic explainability methods, with an emphasis on anomalous fraud detection applications. We also provide:
\begin{itemize} 
    \item A literature review distinguishing dynamic, explainable, and fraud anomaly models from others
    \item A useful and updated taxonomy for filtering and categorizing models
    \item Detailed explanations of explanation model reasoning along with their constraints
    \item Emphasize the potential for applying slightly modified static explainers to dynamic graphs
    \item Recommendations for the evaluation and validation of generated explanations based on model type
    \item A viable experimental method and future research suggestions
\end{itemize}

\subsection{Related Surveys and Differences}
There are few dynamic graph-specific anomaly detection resources, and existing surveys lack time-aware explainability methods. We suggest the following papers presented by category for in-depth background on the development and evolution of transforming, processing, explaining, and validating techniques used in general network anomaly graph analysis:

% feature engineering for data preparation -> traditional models -> deep learning models -> dynamic DL models -> static explainability -> evaluating model+explainers 
\begin{itemize}
    \item Zhang, Yin et al.\cite{zhang_network_2020} review and discuss representation learning as feature engineering, performing empirical studies to compare performance on common datasets.
    \item Akoglu et al.\cite{akoglu_graph_2015} focus on traditional static and dynamic graph anomaly detection methods, placing emphasis on anomaly attribution in a more focused attempt to detect root causes backed by real-world examples.
    \item Ma et al.\cite{ma_comprehensive_2021} provide a comprehensive view of both traditional and deep learning anomaly detection, but make only light mention of dynamic deep learning.
    \item Yuan et al.\cite{yuan_explainability_2023} gather and benchmark static graph explainability methods.
    \item Zhang, Wu et al.\cite{zhang_trustworthy_2022} provide a complete framework for evaluating GNN models holistically, creating separate categories for evaluating all aspects of what they refer to as "trustworthy GNNs".
\end{itemize}

\subsection{Notation and Terminology}
\subsubsection{Graph Types} % this has a hanging vspace
A graph consists of a set of vertices (as nodes) interconnected by a set of edges. A dynamic graph is a superset of this structure that changes over time, evolving with the appearance and removal of edges and nodes. This more closely captures the dynamic nature of real-world systems, where relationships between entities may change over time. In practical terms, a group of people can be mapped to a social network graph using each person to represent a node while their associations (as friends, family, or colleagues) are mapped to the edges connecting them. The details of these relationships can be stored as metadata (called features) in the nodes or on each edge. Extending this static graph to record how associations change over time (e.g., finding a new job, meeting new people, getting married) would yield either a dynamic graph or a static graph with temporal features.

In this context, a temporal graph is a variation of a static graph. The topology of the graph remains fixed, but the features contained within its nodes and edges are able vary over time. A transportation network represented as a temporal graph may have a static set of locations and roads connecting them, but attributes (like road conditions or traffic congestion) may change over time.

Since different graph structures are useful for different tasks, formation of time-aware graph data from unstructured or partially-formed datasets depends on semantics and usefulness. Consider data from a network of mobile phone calls between people: forming a dynamic graph would be suitable for tracking the creation and deletion of calls between individuals, whereas a temporal graph may capture the evolution of the duration, frequency, and location of these calls.

Casteigts et al.\cite{casteigts_time-varying_2012} and others use time-varying graphs (or TVG) as an umbrella term for all graphs that store temporal graphs and refer to sets of static graph snapshots as evolving graphs. Outside of literature, temporal and dynamic are sometimes used interchangeably. In this paper, we distinguish dynamic and temporal graphs based on the appearance and removal of edges and nodes, and the evolution of features. In other words, the time variation of \textit{nodes and edges} is dynamic, while time variation of \textit{features} is temporal. A graph exhibiting both characteristics is named explicitly as a dynamic temporal graph. If left unspecified, static-ness is assumed.

\subsubsection{Hypergraphs}
Hypergraphs are an abstraction of these principles, sometimes used in datasets\cite{benson_cornell_nodate} and graph analysis models\cite{toshniwal_hypergraph_2021} to allow for more flexible representation of the pairwise node-edge relationships represented in traditional graphs. Instead of directedness or undirectedness, hyperedges connect any number of vertices in order to capture higher-order relationships involving sets of entities. Methods using these aren't catalogued in our survey, but they are novel in literature and may influence future dynamic explainability.

\subsubsection{Explanation vs Interpretation}
\textbf{Interpretability} generally refers to the subjective human understanding of an internally transparent model (i.e., a model with clearly defined reasoning for each particular generated result). It involves evaluating a model's design and operation against how its components interact to produce output\cite{zhang_trustworthy_2022}. Excepting "transparency-by-design" models and attention-based interpreters, this means that no machine learning models or explainers are interpretable. 

\textbf{Explainability} is the analysis of these non-transparent "black box" machine learning models. Graph models are classified as black boxes if they generate output from obfuscated intermediate inputs (i.e., its inner workings cannot be clearly followed from input to process to specific result). This means that any explanations for the reasoning behind a particular output must be inferred "post-hoc" after results have been generated.

This is relevant because explanation and interpretation are often used as synonyms in this context. Since each term can be narrowly defined with separate and distinct characteristics, we refer to them separately: interpretation is the attempt to understand and interpret a \textit{knowable} process, while explanation seeks to understand and explain an \textit{unknowable} process.

It is worth noting that attention-based mechanisms are commonly grouped with other explainable methods. We propose categorizing them as interpretable methods instead, since attention mechanisms are based on calculated weights for model input elements during detection training. The final interpreted results are scored when the prediction model is finished, but we do not believe this is sufficient to categorize it as post-hoc. Given that both dynamic graph models and their explanations are still in early development, a standardized vocabulary and framework has not yet been adopted.

\subsubsection{Storing Temporal Information} %TODO Taking too long
Time information can be captured in graphs as a feature via node and edge timestamps, entire graph snapshots taken over a time interval, or as an event. This time information can then be encoded directly through neural networks or dimensionality reduction methods into lower-dimensional vector space via embeddings. Information specifically stored in timestamps can be grouped for further processing via:
\begin{itemize}
    \item \textit{Timestamp aggregation}\cite{george_time-aggregated_2006}: More precise timestamped information can be combined into a less precise time interval in order to simplify graph structure (hourly -> daily, weekly -> monthly) and reduce complexity. 
    \item \textit{Graph slicing}\cite{crichton_neural_2018}: Timestamps can be grouped into desired discrete time intervals by creating a sequence of subgraphs. This maintains precision but increases data structure complexity.
\end{itemize}