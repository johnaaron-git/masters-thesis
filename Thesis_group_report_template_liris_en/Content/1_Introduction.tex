
Dynamic graphs have become increasingly popular in recent years as they provide a flexible and powerful framework for representing complex systems that evolve over time. These graphs have applications in a wide range of fields, from social networks to biological systems, and are used to model the interactions and relationships between different entities. However, as dynamic graphs become more complex, understanding and interpreting them can be challenging. Explainability, or the ability to understand why a particular result or decision was reached, is crucial for many applications, including fraud detection, recommendation systems, and predictive analytics. Despite the growing importance of explainability in dynamic graphs, there is a lack of comprehensive surveys in the field. This survey aims to fill this gap by exploring the state of the art in explainability in dynamic graphs and identifying key challenges and opportunities for future research.


\subsection{Graph description}
A dynamic graph is a mathematical structure that evolves over time, with the appearance and removal of edges and nodes. It is a type of graph that captures the dynamic nature of real-world systems, where relationships between entities may change over time. In a dynamic graph, edges and nodes are not static and can change their state or disappear over time. For example, social networks can be represented as dynamic graphs, where the nodes are people and the edges are the relationships between them, such as friendships or collaborations.

On the other hand, a temporal graph is a static graph where the nodes and edges have attributes that change over time. In this case, the topology of the graph is fixed, but the properties of the nodes and edges vary over time. For instance, a transportation network can be represented as a temporal graph, where the nodes are locations, and the edges are roads or routes connecting them. The attributes of nodes and edges could include travel time, congestion, or availability, which change over time.

To illustrate the difference between dynamic graphs and temporal graphs, consider a network of mobile phone calls between people. A dynamic graph would track the creation and deletion of calls between individuals, while a temporal graph would capture the evolution of the duration, frequency, and location of these calls.

In this paper, we propose to distinguish between dynamic and temporal graphs based on the definition that a dynamic graph refers to the appearance and removal of edges and nodes, while a temporal graph refers to the evolution of features. We also suggest that static graphs with temporal features should be referred to as temporal graphs, while dynamic graphs with static features should be called dynamic graphs. In other words, the time variation of edges and node is called 'dynamic', while it is called 'temporal' for features. Moreover, if not specified, staticness is assumed.

\subsection{Explainability vs Interpretability}

Explainability refers to the post-hoc analysis of black boxes, meaning that the model is treated as a "black box" whose inner workings are not initially understood, but after the model generates output, explanations are derived to understand the reasoning behind the output. The explanation process is done after the fact, rather than during the model's development. In contrast, interpretability refers to the intrinsic human understandable model, meaning that the model is transparent and interpretable from the outset. Interpretability involves understanding the model's design and operation, and how its components interact to produce output.\cite{trustworthy}

On the other hand, interpretability in GNNs refers to the ability to understand how the model operates and how its components interact. This can be achieved by using interpretable architectures, such as graph convolutional networks (GCNs), that allow for a more intuitive understanding of the model's behavior.

In summary, while explainability involves understanding a model's output after it is generated, interpretability refers to the ability to understand the model's inner workings and design from the outset. Both are essential in graph and network analysis, particularly in the context of complex models such as dynamic GNNs. We will use the term explanation as a combination of explanability and interpretability.

The distinction between interpretability and explainability is not always clear-cut, and the terms are often used interchangeably.  For instance, attention mechanisms. They are sometimes referred as interpretability and sometimes as explainability. In this paper, we place it as interpretability since even though it give post-hoc explanation, the weights are calulated during the detection training and are intrinsacally linked to the prediction model.



Dynamic graph and dynamic explainability being very novel domains, a common vocabulary and structure as yet to appear. Most of the papers appeared in Summer 2022. For this reason, we proposed the above definitions to facilitate future work.

\subsection{Accuracy vs. explanation}
The application of machine learning techniques to the domain of algorithmic anomaly detection (e.g., fraud detection) has created the need for additional explanatory layers to interpret and verify results. Traditional, non-deep learning anomaly detection methods transform graph data transparently, but struggle to effectively capture and represent their more complex and interdependent features. Deep learning algorithms provide a scalable and robust alternative at the cost of interpretability, operating as black boxes. The field of ‘explainability’ attempts to remedy this through analysis of deep learning model characteristics but has been largely constrained to static graphs.

\subsection{Embedding approach}
Approaches for analyzing dynamic graphs usually build upon those for static graphs, but with added consideration for temporal dimensions and update methods. For instance, matrix factorization-based methods rely on eigenvectors of the graph Laplacian matrix to generate node embeddings, which can be updated efficiently using prior embeddings in DANE. Meanwhile, random walk-based approaches use the inner products of node embeddings to model transition probabilities conditioned on history, such as in CTDANE and NetWalk. The wave of deep learning has also introduced unsupervised and supervised methods, such as DynGEM, which is an autoencoding approach that minimizes the reconstruction loss and uses an adaptive architecture depth that is initialized with previous time steps. Another category is point processes that are continuous in time, like KnowEvolve and DyRep, which model the occurrence of an edge as a point process and use neural networks to parameterize the intensity function. Combinations of graph neural networks (GNNs) and recurrent architectures, such as LSTM, have also been explored, particularly with graph convolutional networks (GCNs), as seen in GCRN, WD-GCN/CD-GCN, RgCNN, and STGCN. These approaches use graph convolution layers to modify the LSTM or perform spatiotemporal convolutions for evolving node features. STGCN was specifically designed for spatiotemporal traffic data using graph convolutions to handle spatial information.\cite{egcn}



Review that focuses on deep learning models that are designed to handle data with both temporal and spatial components exists. These models are typically created by combining existing neural network layers designed for sequential and static graph-structured data.

Temporal deep learning involves generating in-memory representations of data points using techniques such as LSTM and GRU. These representations are iteratively updated as new data is processed. Alternatively, some temporal deep learning models use an attention mechanism to adaptively re-contextualize representations based on the temporal history of the data.

The second type of model, known as static graph representation learning, involves learning representations of vertices, edges, and whole graphs using graph neural networks. This is done by generating compressed representations (messages) of node and edge attributes, which are propagated between nodes based on a message-passing rule and then aggregated to form new representations. Several existing graph neural network architectures fit this general description, differing mainly in their assumptions about the input graph and the specific compression, propagation, and aggregation functions they use.

Finally, spatiotemporal deep learning models combine the basic ideas of temporal deep learning and graph representation learning to work with temporal graph sequences. These models perform message-passing at each time point using a graph neural network block and incorporate new temporal information using a temporal deep learning block. By doing so, these models can take advantage of the shared temporal and spatial autocorrelation information across spatial units. The temporal and spatial layers of these models are fused together in a single parametric machine learning model that is trained jointly, exploiting the end-to-end differentiability of the fused models. Table 1 summarizes the spatiotemporal deep learning models we have implemented in our framework, categorized based on the type of temporal and graph neural network layer blocks used, as well as the order of spatial proximity and heterogeneity of the edge set.\cite{rozemberczki2021pytorch}



